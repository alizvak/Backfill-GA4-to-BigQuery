{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoGfjTZ8vo7AO2AjltEWSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alizvak/Backfill-GA4-to-BigQuery/blob/main/AIOA_PUP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koEm275SR-Un",
        "outputId": "e8bb05a7-290f-4a26-e814-a23d685ffd98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-analytics-data==0.18.4 in /usr/local/lib/python3.10/dist-packages (0.18.4)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-analytics-data==0.18.4) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-analytics-data==0.18.4) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-analytics-data==0.18.4) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (2.27.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (1.63.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (2024.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-analytics-data==0.18.4) (0.6.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0) (0.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (2.27.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (4.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.31.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-auth-oauthlib) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2024.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (2.27.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.1.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-auth-httplib2) (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-analytics-data==0.18.4\n",
        "!pip install google-cloud-bigquery\n",
        "!pip install google-auth==2.27.0\n",
        "!pip install google-auth-oauthlib\n",
        "!pip install google-auth-httplib2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='country'),\n",
        "                    Dimension(name='language')\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers')\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:country'},\n",
        "                                   {'name': 'ga:language'}\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:pageviews': 'screenPageViews',\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "            dim2 = row.dimension_values[2].value\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'country': dim1,\n",
        "                'language': dim2,\n",
        "                'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'screenPageViews': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "                'newUsers': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "                dim2 = row['dimensions'][2]\n",
        "                row_data = {'date': date_value, 'country': dim1 , 'language': dim2 }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'sessions': 'int64',\n",
        "        'screenPageViews': 'int64',\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"country\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"language\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_geo_network'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxJiHU_5SYIl",
        "outputId": "29ffb3c6-2f82-4a89-e0d2-42a84559dda4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date               datetime64[ns]\n",
            "country                    object\n",
            "language                   object\n",
            "sessions                    int64\n",
            "screenPageViews             int64\n",
            "totalUsers                  int64\n",
            "newUsers                    int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_geo_network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='sessionSource'),\n",
        "                    Dimension(name='eventName'),\n",
        "                    Dimension(name='sessionMedium')],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='eventCount'),\n",
        "            Metric(name='totalUsers')\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:goal4Completions'},\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:source'},\n",
        "                                   {'name': 'ga:hostname'},\n",
        "                                   {'name': 'ga:medium'},],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:goal4Completions': 'eventCount'   }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "            dim2 = row.dimension_values[2].value\n",
        "            dim3 = row.dimension_values[3].value\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'sessionSource': dim1,\n",
        "                'eventName': dim2,\n",
        "                'sessionMedium': dim3,\n",
        "                'eventCount': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "                dim2 = row['dimensions'][2]\n",
        "                dim3 = row['dimensions'][3]\n",
        "                row_data = {'date': date_value, 'sessionSource': dim1 , 'eventName': dim2 , 'sessionMedium': dim3}\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'eventCount': 'int64'   })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "    # Schema and upload as before\n",
        "    # You can continue from here...\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionSource\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"eventName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionMedium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"eventCount\", \"INTEGER\")   ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_goal_events'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KYA9gJjScdB",
        "outputId": "c9523070-ce17-4012-94f3-82e962295534"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date             datetime64[ns]\n",
            "sessionSource            object\n",
            "eventName                object\n",
            "sessionMedium            object\n",
            "eventCount                int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_goal_events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='itemName')\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "\n",
        "            Metric(name='itemPurchaseQuantity'),\n",
        "            Metric(name='itemRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "\n",
        "                        {'expression': 'ga:itemQuantity'},\n",
        "                        {'expression': 'ga:itemRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:productName'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "\n",
        "        'ga:itemQuantity': 'itemPurchaseQuantity',\n",
        "        'ga:itemRevenue': 'itemRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'itemName': dim1,\n",
        "\n",
        "                'itemPurchaseQuantity': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'itemRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "\n",
        "                row_data = {'date': date_value, 'itemName': dim1 }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "\n",
        "        'itemPurchaseQuantity': 'int64',\n",
        "        'itemRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"itemName\", \"STRING\"),\n",
        "\n",
        "        bigquery.SchemaField(\"itemPurchaseQuantity\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"itemRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_items'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7jPJSGKSg2y",
        "outputId": "f588bb5d-b449-45d6-a17b-793a3eb3139d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 'date'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='landingPage')\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:landingPagePath'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers',\n",
        "        'ga:uniquePurchases': 'ecommercePurchases',\n",
        "        'ga:transactionRevenue': 'purchaseRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'landingPage': dim1,\n",
        "\n",
        "                'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "                 'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "                'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "                'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "\n",
        "                row_data = {'date': date_value, 'landingPage': dim1 }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'sessions': 'int64',\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64',\n",
        "        'ecommercePurchases': 'int64',\n",
        "        'purchaseRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"landingPage\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_landingpagepath'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woR-pQexSmgf",
        "outputId": "3e1b59fc-1a20-4904-a71c-bbd38ee7a1e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                  datetime64[ns]\n",
            "landingPage                   object\n",
            "sessions                       int64\n",
            "totalUsers                     int64\n",
            "newUsers                       int64\n",
            "ecommercePurchases             int64\n",
            "purchaseRevenue              float64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_landingpagepath\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='pagePathPlusQueryString')\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='totalUsers')\n",
        "\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'}\n",
        "\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:pagePath'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:pageviews': 'screenPageViews',\n",
        "        'ga:users': 'totalUsers'\n",
        "\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'pagePathPlusQueryString': dim1,\n",
        "\n",
        "                'screenPageViews': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "\n",
        "                row_data = {'date': date_value, 'pagePathPlusQueryString': dim1 }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'screenPageViews': 'int64',\n",
        "        'totalUsers': 'int64'\n",
        "\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"pagePathPlusQueryString\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\")\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_pagepath'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "-VO1wyL1So2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')],\n",
        "        metrics=[Metric(name='screenPageViews')],\n",
        "        limit=100000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [{'expression': 'ga:pageviews'}],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    if api_type == 'GA4':\n",
        "        list_rows = []\n",
        "        for row in response.rows:\n",
        "            row_data = {\n",
        "                'date': row.dimension_values[0].value,\n",
        "                'screenPageViews': int(row.metric_values[0].value)\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "        return pd.DataFrame(list_rows)\n",
        "    else:\n",
        "        list_rows = []\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "                for metric, values in zip(metricHeaders, row['metrics'][0]['values']):\n",
        "                    row_data[metric['name']] = values\n",
        "                list_rows.append(row_data)\n",
        "        return pd.DataFrame(list_rows)\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Standardize column names for UA\n",
        "    ua_df.columns = ['date', 'screenPageViews']\n",
        "\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date']).dt.date  # Ensuring date format\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    unified_df['screenPageViews'] = unified_df['screenPageViews'].astype(int)  # Ensuring integers\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_pageviews'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4UF4hI5S0Jz",
        "outputId": "e7814344-352e-44cf-a316-96e27a6d2be5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date               object\n",
            "screenPageViews     int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_pageviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='browser'),\n",
        "                    Dimension(name='operatingSystem'),\n",
        "                    Dimension(name='deviceCategory')],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='screenPageViews'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers')\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:browser'},\n",
        "                                   {'name': 'ga:operatingSystem'},\n",
        "                                   {'name': 'ga:deviceCategory'},],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:pageviews': 'screenPageViews',\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "            dim2 = row.dimension_values[2].value\n",
        "            dim3 = row.dimension_values[3].value\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'browser': dim1,\n",
        "                'operatingSystem': dim2,\n",
        "                'deviceCategory': dim3,\n",
        "                'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'screenPageViews': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "                'newUsers': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "                dim2 = row['dimensions'][2]\n",
        "                dim3 = row['dimensions'][3]\n",
        "                row_data = {'date': date_value, 'browser': dim1 , 'operatingSystem': dim2 , 'deviceCategory': dim3}\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'sessions': 'int64',\n",
        "        'screenPageViews': 'int64',\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"browser\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"operatingSystem\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"deviceCategory\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"screenPageViews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_platform_device'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HgAvDxES95N",
        "outputId": "73f20341-6cb2-49a8-d9de-2125be6d1fda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date               datetime64[ns]\n",
            "browser                    object\n",
            "operatingSystem            object\n",
            "deviceCategory             object\n",
            "sessions                    int64\n",
            "screenPageViews             int64\n",
            "totalUsers                  int64\n",
            "newUsers                    int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_platform_device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')\n",
        "\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}\n",
        "\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "\n",
        "        'ga:uniquePurchases': 'ecommercePurchases',\n",
        "        'ga:transactionRevenue': 'purchaseRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "\n",
        "\n",
        "                'ecommercePurchases': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'purchaseRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "\n",
        "\n",
        "                row_data = {'date': date_value }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "\n",
        "        'ecommercePurchases': 'int64',\n",
        "        'purchaseRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_purchases'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFD_aa4IS_i5",
        "outputId": "42193a66-4927-42f1-f643-f75066277f6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 'date'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')\n",
        "\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}\n",
        "\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "\n",
        "        'ga:uniquePurchases': 'ecommercePurchases',\n",
        "        'ga:transactionRevenue': 'purchaseRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "\n",
        "\n",
        "                'ecommercePurchases': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'purchaseRevenue': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "\n",
        "\n",
        "                row_data = {'date': date_value }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "\n",
        "        'ecommercePurchases': 'int64',\n",
        "        'purchaseRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_purchases'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gwD4Q1OTWa8",
        "outputId": "f31cd2e8-ed2c-4e1d-b94f-f9ae02bd5c3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 'date'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='sessionPrimaryChannelGroup')\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:channelGrouping'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers',\n",
        "        'ga:uniquePurchases': 'ecommercePurchases',\n",
        "        'ga:transactionRevenue': 'purchaseRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'sessionPrimaryChannelGroup': dim1,\n",
        "\n",
        "                'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "                'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "                'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "                'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "\n",
        "                row_data = {'date': date_value, 'sessionPrimaryChannelGroup': dim1 }\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'sessions': 'int64',\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64',\n",
        "        'ecommercePurchases': 'int64',\n",
        "        'purchaseRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionPrimaryChannelGroup\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_session_channel_group'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRrbgk7DTZ4K",
        "outputId": "75cf14fc-a00c-419c-90f1-a6112fca913e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                          datetime64[ns]\n",
            "sessionPrimaryChannelGroup            object\n",
            "sessions                               int64\n",
            "totalUsers                             int64\n",
            "newUsers                               int64\n",
            "ecommercePurchases                     int64\n",
            "purchaseRevenue                      float64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_session_channel_group\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')],\n",
        "        metrics=[Metric(name='sessions')],\n",
        "        limit=100000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [{'expression': 'ga:sessions'}],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    if api_type == 'GA4':\n",
        "        list_rows = []\n",
        "        for row in response.rows:\n",
        "            row_data = {\n",
        "                'date': row.dimension_values[0].value,\n",
        "                'Sessions': int(row.metric_values[0].value)\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "        return pd.DataFrame(list_rows)\n",
        "    else:\n",
        "        list_rows = []\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "                for metric, values in zip(metricHeaders, row['metrics'][0]['values']):\n",
        "                    row_data[metric['name']] = values\n",
        "                list_rows.append(row_data)\n",
        "        return pd.DataFrame(list_rows)\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Standardize column names for UA\n",
        "    ua_df.columns = ['date', 'Sessions']\n",
        "\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date']).dt.date  # Ensuring date format\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    unified_df['Sessions'] = unified_df['Sessions'].astype(int)  # Ensuring integers\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"Sessions\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_sessions'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu4Y69ZOTkDn",
        "outputId": "e7edce98-0827-45b2-bc8d-7f8bfb9b73a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date        object\n",
            "Sessions     int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_sessions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date'),\n",
        "                    Dimension(name='sessionSource'), Dimension(name='sessionCampaignName'), Dimension(name='sessionMedium')\n",
        "\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='sessions'),\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers'),\n",
        "            Metric(name='ecommercePurchases'),\n",
        "            Metric(name='purchaseRevenue'),\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:source'},{'name': 'ga:campaign'},{'name': 'ga:medium'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers',\n",
        "        'ga:uniquePurchases': 'ecommercePurchases',\n",
        "        'ga:transactionRevenue': 'purchaseRevenue'\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "            dim1 = row.dimension_values[1].value\n",
        "            dim2 = row.dimension_values[2].value\n",
        "            dim3 = row.dimension_values[3].value\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "                'sessionSource': dim1,\n",
        "                'sessionCampaignName': dim2,\n",
        "                'sessionMedium': dim3,\n",
        "\n",
        "                'sessions': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0,\n",
        "                 'newUsers': pd.to_numeric(row.metric_values[2].value, errors='coerce') or 0,\n",
        "                'ecommercePurchases': pd.to_numeric(row.metric_values[3].value, errors='coerce') or 0,\n",
        "                'purchaseRevenue': pd.to_numeric(row.metric_values[4].value, errors='coerce') or 0\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "                dim1 = row['dimensions'][1]\n",
        "                dim2 = row['dimensions'][2]\n",
        "                dim3 = row['dimensions'][3]\n",
        "\n",
        "                row_data = {'date': date_value,'sessionSource': dim1,\n",
        "                'sessionCampaignName': dim2,\n",
        "                'sessionMedium': dim3}\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "        'sessions': 'int64',\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64',\n",
        "        'ecommercePurchases': 'int64',\n",
        "        'purchaseRevenue': 'float'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessionSource\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionCampaignName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessionMedium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_source_campaign_medium'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkg9cHQRTqGS",
        "outputId": "0842bca6-e050-46f6-d680-d6ac6bca1555"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                   datetime64[ns]\n",
            "sessionSource                  object\n",
            "sessionCampaignName            object\n",
            "sessionMedium                  object\n",
            "sessions                        int64\n",
            "totalUsers                      int64\n",
            "newUsers                        int64\n",
            "ecommercePurchases              int64\n",
            "purchaseRevenue               float64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_source_campaign_medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "from google.analytics.data_v1beta.types import DateRange, Dimension, Metric, RunReportRequest\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# GA4 Authentication and Client Setup\n",
        "SCOPES_GA = ['https://www.googleapis.com/auth/analytics.readonly', 'https://www.googleapis.com/auth/bigquery']\n",
        "creds_ga = service_account.Credentials.from_service_account_file(\n",
        "    config['SERVICE_ACCOUNT_FILE'], scopes=SCOPES_GA)\n",
        "bq_client = bigquery.Client(credentials=creds_ga, project=config['BIGQUERY_PROJECT'])\n",
        "client_ga4 = BetaAnalyticsDataClient(credentials=creds_ga)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ga4_report():\n",
        "    \"\"\"Fetches GA4 data based on the defined dimensions and metrics.\"\"\"\n",
        "    request = RunReportRequest(\n",
        "        property=f'properties/{config[\"PROPERTY_ID_GA4\"]}',\n",
        "        date_ranges=[DateRange(start_date=config['INITIAL_FETCH_FROM_DATE'], end_date=config['FETCH_TO_DATE'])],\n",
        "        dimensions=[Dimension(name='date')\n",
        "                    ],\n",
        "        metrics=[\n",
        "            Metric(name='totalUsers'),\n",
        "            Metric(name='newUsers')\n",
        "\n",
        "        ],\n",
        "        limit=10000000\n",
        "    )\n",
        "    return client_ga4.run_report(request)\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}\n",
        "\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response, api_type='GA4'):\n",
        "    list_rows = []\n",
        "    metric_map = {\n",
        "\n",
        "        'ga:users': 'totalUsers',\n",
        "        'ga:newUsers': 'newUsers'\n",
        "\n",
        "    }\n",
        "\n",
        "    if api_type == 'GA4':\n",
        "        for row in response.rows:\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row.dimension_values[0].value, format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT  # Use Not-a-Time for dates that fail to convert\n",
        "\n",
        "\n",
        "            list_rows.append({\n",
        "                'date': date_value,\n",
        "\n",
        "                'totalUsers': pd.to_numeric(row.metric_values[0].value, errors='coerce') or 0,\n",
        "                 'newUsers': pd.to_numeric(row.metric_values[1].value, errors='coerce') or 0\n",
        "\n",
        "\n",
        "            })\n",
        "    else:\n",
        "        for report in response.get('reports', []):\n",
        "            columnHeader = report.get('columnHeader', {})\n",
        "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "            for row in report.get('data', {}).get('rows', []):\n",
        "                try:\n",
        "                    date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "                except ValueError:\n",
        "                    date_value = pd.NaT\n",
        "\n",
        "                row_data = {'date': date_value}\n",
        "                for i, metric in enumerate(metricHeaders):\n",
        "                    mapped_name = metric_map.get(metric['name'])\n",
        "                    if mapped_name:\n",
        "                        row_data[mapped_name] = pd.to_numeric(row['metrics'][0]['values'][i], errors='coerce', downcast='float') or 0\n",
        "                list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "\n",
        "def unify_and_upload_data(ga4_df, ua_df):\n",
        "    # Concatenate GA4 and UA data\n",
        "    unified_df = pd.concat([ga4_df, ua_df], ignore_index=True)\n",
        "\n",
        "    # Fill NaN values with 0 to maintain uniformity\n",
        "    unified_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure 'date' remains as date object and not converted mistakenly\n",
        "    unified_df['date'] = pd.to_datetime(unified_df['date'])\n",
        "\n",
        "    # Convert data types\n",
        "    unified_df = unified_df.astype({\n",
        "\n",
        "        'totalUsers': 'int64',\n",
        "        'newUsers': 'int64'\n",
        "    })\n",
        "\n",
        "    print(unified_df.dtypes)\n",
        "\n",
        "\n",
        "    # Create schema for the unified DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_id = 'unified_data_users_newusers'\n",
        "    table_ref = f\"{bq_client.project}.{BIGQUERY_DATASET}.{table_id}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client.load_table_from_dataframe(unified_df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Unified data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # GA4 processing\n",
        "        ga4_response = get_ga4_report()\n",
        "        ga4_df = response_to_dataframe(ga4_response, api_type='GA4')\n",
        "\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response, api_type='UA')\n",
        "\n",
        "        # Unify and upload the data\n",
        "        unify_and_upload_data(ga4_df, ua_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VSZadc_UFmD",
        "outputId": "cacde354-1625-43f9-cdff-62fed4456249"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date          datetime64[ns]\n",
            "totalUsers             int64\n",
            "newUsers               int64\n",
            "dtype: object\n",
            "Unified data uploaded and partitioned by date to pup-data-warehouse.Siavak_Backfill_Data.unified_data_users_newusers\n"
          ]
        }
      ]
    }
  ]
}